### Hi there ðŸ‘‹
My name is Haojun Jiang(è’‹æ˜Šå³»), a third-year Ph.D. student in the Department of Automation at Tsinghua University, advised by Prof. [Gao Huang](http://www.gaohuang.net/). Before that, I received my B.E. degree in Automation at Tsinghua University. ðŸ˜„

Iâ€™m currently working on vision and language.

<!--**jianghaojun/jianghaojun** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ðŸ”­ Iâ€™m currently working on ...
- ðŸŒ± Iâ€™m currently learning ...
- ðŸ‘¯ Iâ€™m looking to collaborate on ...
- ðŸ¤” Iâ€™m looking for help with ...
- ðŸ’¬ Ask me about ...
- ðŸ“« How to reach me: ...
- ðŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->

### ðŸ’¬ News
**[2023/07]**: **Deep Incubation: Training Large Models by Divide-and-Conquering**(https://arxiv.org/abs/2212.04129) is accepted by ICCV 2023! Paper is available at [arXiv](https://arxiv.org/abs/2212.04129).    
**[2023/01]**: **Text4Point** now is available at [arXiv](https://arxiv.org/abs/2301.07584). This work propose a novel Text4Point framework to construct **language-guided 3D point cloud models**. The key idea is utilizing 2D images as a bridge to connect the point cloud and the language modalities.    
**[2022/12]**: A curated list about [Parameter Efficient Transfer Learning](https://github.com/jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning) in computer vision and multimodal is created.  
**[2022/12]**: **Deep Incubation: Training Large Models by Divide-and-Conquering** now is available at [arXiv](https://arxiv.org/abs/2212.04129). This work explores a novel **Modular Training** paradigm which divides a large model into smaller modules, trains them independently, and reassembles the trained modules to obtain the target model.  
**[2022/11]**: **Cross-Modal Adpater** now is available at [arXiv](https://arxiv.org/abs/2211.09623). This work explores the **adapter-based parameter-efficient transfer learning** for text-video retrieval domain. It reduces **99.6\%** of fine-tuned parameters without performance degradation.  
**[2022/09]**: An introduction about [Parameter Efficient Transfer Learning](https://cloud.tsinghua.edu.cn/f/73309dec3ea3496db459/?dl=1) is given in [BAAI](https://www.baai.ac.cn/english.html) dynamic neural network seminar.  
**[2022/07]**: [Glance and Focus Networks for Dynamic Visual Recognition](https://arxiv.org/pdf/2201.03014.pdf) is accepted by TPAMI (IF=24.31)!  
**[2022/07]**: [AI Time](http://www.aitime.cn/) invites me to give a talk about [Pseudo-Q](https://www.bilibili.com/video/BV1LB4y1e7kT?spm_id_from=333.337.search-card.all.click&vd_source=17f8133aaca9f7f8e61c08b61e26d162).  
**[2022/04]**: An introduction about [3D Visual Grounding](https://cloud.tsinghua.edu.cn/f/31f0f6930817424db210/?dl=1) is given in [BAAI](https://www.baai.ac.cn/english.html) dynamic neural network seminar.  
**[2022/04]**: A curated list about [3D Vision and Language](https://github.com/jianghaojun/Awesome-3D-Visual-Grounding) is created.  
**[2022/03]**: [Pseudo-Q](https://arxiv.org/abs/2203.08481) and [AdaFocusV2](https://arxiv.org/abs/2112.14238) are accepted by CVPR 2022!  
**[2021/07]**: [AdaFocus](https://arxiv.org/abs/2105.03245) is accepted by ICCV 2021!  
**[2021/03]**: [CondenseNetV2](https://arxiv.org/abs/2104.04382) is accepted by CVPR 2021!  

### ðŸŒ± Academic Services 
- **Conference Reviewer**: CVPR 2022-2023, ICCV 2023, ECCV 2022

### ðŸ“« Contact 
My email is jhj20 at mails.tsinghua.edu.cn. 

### âœ¨ GitHub Stats 
[![Haojun's GitHub stats](https://github-readme-stats.vercel.app/api?username=jianghaojun&show_icons=true&theme=tokyonight)](https://github.com/anuraghazra/github-readme-stats)

<!-- ### Visitors -->
<!-- <p align="left"> 
  <img src="https://profile-counter.glitch.me/jianghaojun/count.svg" />
</p> -->
<!-- <a href="https://www.easycounter.com/">
<img src="https://www.easycounter.com/counter.php?jhj20"
border="0" alt="stats counter"></a>
<br><a href="https://www.easycounter.com/"></a>
 -->
